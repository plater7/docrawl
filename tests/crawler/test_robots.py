"""
Unit tests for src/crawler/robots.py

Generated by OpenCode ðŸ¤– (AI assistant) with human review.
This code was created through AI-assisted development and reviewed by a human developer.

Tests cover:
- robots.txt parsing
- Disallow rules
- Crawl-delay parsing
- is_allowed checks
- Error handling
"""

import pytest
from unittest.mock import AsyncMock, patch
import httpx

from src.crawler.robots import RobotsParser


class TestRobotsParser:
    """Test RobotsParser class."""

    def test_init(self):
        """Parser should initialize with empty state."""
        parser = RobotsParser()
        assert parser.disallowed == []
        assert parser.crawl_delay is None


class TestParseRobotsTxt:
    """Test robots.txt content parsing."""

    def test_parse_simple_disallow(self):
        """Simple disallow rule should be parsed."""
        parser = RobotsParser()
        content = """User-agent: *
Disallow: /private/
Disallow: /admin/
"""
        parser._parse(content)
        assert "/private/" in parser.disallowed
        assert "/admin/" in parser.disallowed

    def test_parse_crawl_delay(self):
        """Crawl-delay should be parsed."""
        parser = RobotsParser()
        content = """User-agent: *
Crawl-delay: 10
"""
        parser._parse(content)
        assert parser.crawl_delay == 10.0

    def test_parse_only_user_agent_all(self):
        """Only rules under User-agent: * should be applied."""
        parser = RobotsParser()
        content = """User-agent: GoogleBot
Disallow: /google-only/

User-agent: *
Disallow: /all-bots/
"""
        parser._parse(content)
        assert "/all-bots/" in parser.disallowed
        assert "/google-only/" not in parser.disallowed

    def test_parse_empty_disallow(self):
        """Empty disallow should not add to list."""
        parser = RobotsParser()
        content = """User-agent: *
Disallow:
"""
        parser._parse(content)
        assert parser.disallowed == []

    def test_parse_multiple_user_agent_sections(self):
        """Multiple user-agent sections should be handled."""
        parser = RobotsParser()
        content = """User-agent: GoogleBot
Disallow: /google/

User-agent: *
Disallow: /all/

User-agent: BingBot
Disallow: /bing/
"""
        parser._parse(content)
        assert "/all/" in parser.disallowed
        assert len(parser.disallowed) == 1

    def test_parse_case_insensitive(self):
        """Parsing should be case-insensitive."""
        parser = RobotsParser()
        content = """USER-AGENT: *
DISALLOW: /private/
CRAWL-DELAY: 5
"""
        parser._parse(content)
        assert "/private/" in parser.disallowed
        assert parser.crawl_delay == 5.0

    def test_parse_invalid_crawl_delay(self):
        """Invalid crawl-delay should be ignored."""
        parser = RobotsParser()
        content = """User-agent: *
Crawl-delay: invalid
"""
        parser._parse(content)
        assert parser.crawl_delay is None

    def test_parse_complex_robots(self):
        """Complex robots.txt should be parsed correctly."""
        parser = RobotsParser()
        content = """# This is a comment
User-agent: *
Disallow: /private/
Disallow: /admin/
Disallow: /tmp/
Crawl-delay: 5

User-agent: BadBot
Disallow: /
"""
        parser._parse(content)
        assert len(parser.disallowed) == 3
        assert parser.crawl_delay == 5.0


class TestIsAllowed:
    """Test is_allowed method."""

    def test_allowed_path(self):
        """Allowed path should return True."""
        parser = RobotsParser()
        parser.disallowed = ["/private/", "/admin/"]
        assert parser.is_allowed("https://example.com/public/page") is True
        assert parser.is_allowed("https://example.com/docs/guide") is True

    def test_disallowed_path(self):
        """Disallowed path should return False."""
        parser = RobotsParser()
        parser.disallowed = ["/private/", "/admin/"]
        assert parser.is_allowed("https://example.com/private/data") is False
        assert parser.is_allowed("https://example.com/admin/settings") is False

    def test_partial_path_match(self):
        """Path matching should use startswith logic."""
        parser = RobotsParser()
        parser.disallowed = ["/api/"]
        assert parser.is_allowed("https://example.com/api/endpoint") is False
        assert parser.is_allowed("https://example.com/api/v1/users") is False
        assert parser.is_allowed("https://example.com/apidocs") is True

    def test_root_path_allowed(self):
        """Root path should be allowed if not explicitly disallowed."""
        parser = RobotsParser()
        parser.disallowed = ["/private/"]
        assert parser.is_allowed("https://example.com/") is True
        assert parser.is_allowed("https://example.com") is True

    def test_empty_disallow_list(self):
        """Empty disallow list should allow all paths."""
        parser = RobotsParser()
        parser.disallowed = []
        assert parser.is_allowed("https://example.com/any/path") is True

    def test_disallow_root(self):
        """Disallow / should block all paths."""
        parser = RobotsParser()
        parser.disallowed = ["/"]
        assert parser.is_allowed("https://example.com/") is False
        assert parser.is_allowed("https://example.com/page") is False
        assert parser.is_allowed("https://example.com/docs/guide") is False


class TestLoadRobotsTxt:
    """Test loading robots.txt from URL."""

    @pytest.mark.asyncio
    async def test_load_success(self):
        """Successful load should return True and parse content."""
        parser = RobotsParser()
        content = """User-agent: *
Disallow: /private/
"""

        with patch("src.crawler.robots.httpx.AsyncClient") as MockClient:
            mock_response = AsyncMock()
            mock_response.status_code = 200
            mock_response.text = content

            client_instance = AsyncMock()
            client_instance.get.return_value = mock_response
            client_instance.__aenter__ = AsyncMock(return_value=client_instance)
            client_instance.__aexit__ = AsyncMock(return_value=False)
            MockClient.return_value = client_instance

            result = await parser.load("https://example.com/")
            assert result is True
            assert "/private/" in parser.disallowed

    @pytest.mark.asyncio
    async def test_load_404(self):
        """404 response should return False."""
        parser = RobotsParser()

        with patch("src.crawler.robots.httpx.AsyncClient") as MockClient:
            mock_response = AsyncMock()
            mock_response.status_code = 404

            client_instance = AsyncMock()
            client_instance.get.return_value = mock_response
            client_instance.__aenter__ = AsyncMock(return_value=client_instance)
            client_instance.__aexit__ = AsyncMock(return_value=False)
            MockClient.return_value = client_instance

            result = await parser.load("https://example.com/")
            assert result is False

    @pytest.mark.asyncio
    async def test_load_timeout(self):
        """Timeout should be handled gracefully."""
        parser = RobotsParser()

        with patch("src.crawler.robots.httpx.AsyncClient") as MockClient:
            client_instance = AsyncMock()
            client_instance.get.side_effect = httpx.TimeoutException("timeout")
            client_instance.__aenter__ = AsyncMock(return_value=client_instance)
            client_instance.__aexit__ = AsyncMock(return_value=False)
            MockClient.return_value = client_instance

            result = await parser.load("https://example.com/")
            assert result is False

    @pytest.mark.asyncio
    async def test_load_connection_error(self):
        """Connection error should be handled gracefully."""
        parser = RobotsParser()

        with patch("src.crawler.robots.httpx.AsyncClient") as MockClient:
            client_instance = AsyncMock()
            client_instance.get.side_effect = httpx.ConnectError("connection failed")
            client_instance.__aenter__ = AsyncMock(return_value=client_instance)
            client_instance.__aexit__ = AsyncMock(return_value=False)
            MockClient.return_value = client_instance

            result = await parser.load("https://example.com/")
            assert result is False

    @pytest.mark.asyncio
    async def test_load_constructs_correct_url(self):
        """Should construct correct robots.txt URL."""
        parser = RobotsParser()

        with patch("src.crawler.robots.httpx.AsyncClient") as MockClient:
            mock_response = AsyncMock()
            mock_response.status_code = 404

            client_instance = AsyncMock()
            client_instance.get.return_value = mock_response
            client_instance.__aenter__ = AsyncMock(return_value=client_instance)
            client_instance.__aexit__ = AsyncMock(return_value=False)
            MockClient.return_value = client_instance

            await parser.load("https://docs.example.com/guide/")

            client_instance.get.assert_called_once()
            call_args = client_instance.get.call_args
            assert "https://docs.example.com/robots.txt" in str(call_args)


class TestEdgeCases:
    """Test edge cases and special scenarios."""

    def test_parse_empty_content(self):
        """Empty robots.txt should be handled."""
        parser = RobotsParser()
        parser._parse("")
        assert parser.disallowed == []
        assert parser.crawl_delay is None

    def test_parse_whitespace_only(self):
        """Whitespace-only content should be handled."""
        parser = RobotsParser()
        parser._parse("   \n   \n   ")
        assert parser.disallowed == []
        assert parser.crawl_delay is None

    def test_parse_comment_only(self):
        """Comment-only content should be handled."""
        parser = RobotsParser()
        parser._parse("# This is a comment\n# Another comment")
        assert parser.disallowed == []
        assert parser.crawl_delay is None

    def test_is_allowed_with_query_params(self):
        """URLs with query params should be handled."""
        parser = RobotsParser()
        parser.disallowed = ["/private/"]
        assert parser.is_allowed("https://example.com/page?foo=bar") is True
        assert parser.is_allowed("https://example.com/private/data?foo=bar") is False

    def test_is_allowed_with_fragment(self):
        """URLs with fragments should be handled."""
        parser = RobotsParser()
        parser.disallowed = ["/private/"]
        assert parser.is_allowed("https://example.com/page#section") is True
        assert parser.is_allowed("https://example.com/private/data#section") is False

    def test_url_with_port(self):
        """URLs with ports should be handled."""
        parser = RobotsParser()
        parser.disallowed = ["/private/"]
        assert parser.is_allowed("https://example.com:8080/public") is True
        assert parser.is_allowed("https://example.com:8080/private/data") is False
